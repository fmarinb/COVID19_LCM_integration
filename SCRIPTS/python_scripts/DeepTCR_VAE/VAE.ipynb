{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python setup #\n",
    "\n",
    "The work developed using DeepTCR has been carried out in a virtual Python 3.7.17 environment, so that the DeepTCR dependencies are correctly installed, and therefore the library itself. The execution of DeepTCR has been done using Jupyter notebooks in VScode. In addition to DeepTCR it is necessary to import the sys, pandas, numpy and pickle libraries.\n",
    "\n",
    "For this analysis, the DeepTCR_U module has been imported, which has the necessary functions for the training and evaluation of the unsupervised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.append('../../')\n",
    "from DeepTCR.DeepTCR import DeepTCR_U\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised analysis #\n",
    "The unsupervised analysis consists of the training, graphical representation and subsequent testing of the variational autoencoder (VAE). VAE will be used to analyse the CDR3b sequences, V alleles and J alleles of the 1000 most expanded clonotypes of each sample, with the aim of obtaining a latent representation of the samples based on these characteristics. For this work, the VAE model will be trained with Sparsity, which aims to find the minimum number of latent classes that explain 99% of the explained variance. For this purpose, we use the parameters sparsity_alpha=0.1 and var_explained=0.99 within the Train_VAE() function. The hyperparameter sparsity_alpha penalises the VAE model during training and modulates how sparse we want our latent representation to be. In this way, we seek to reduce the number of latent features and thus the risk of collinearity, by way of regularisation. The value of 1.0 is chosen as it is considered by the DeepTCR developers as a good starting point. Para más información, consulte la API de DeepTCR (https://sidhomj.github.io/DeepTCR/api/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Getting data for the VAE analysis of the three clusters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTCR_U = DeepTCR_U('unsuperv')\n",
    "classes = ['ct1', 'ct2', 'ct3'] # Definition of classes of samples we compare\n",
    "DTCR_U.Get_Data('clusters',\n",
    "              Load_Prev_Data=True,\n",
    "              aa_column_beta=0,\n",
    "              v_beta_column=2,\n",
    "              j_beta_column=3,\n",
    "              count_column=1,\n",
    "              data_cut=1000,\n",
    "              type_of_data_cut='Num_Seq', # Selecting to 1000 expanded clonotypes\n",
    "              aggregate_by_aa=True) # Preventing redundancy bias of repeated clonotypes by aminoacid sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. VAE trainning ###\n",
    "\n",
    "We use the Train_VAE() function with the hyperparameters for choosing the number of latent classes explained in the previous text (sparsity_alpha=1.0,var_explained = 0.99)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Done\n"
     ]
    }
   ],
   "source": [
    "# Train VAE model\n",
    "DTCR_U.Train_VAE(Load_Prev_Data=False,sparsity_alpha=1.0,var_explained = 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Graphical representation ###\n",
    "\n",
    "A clear and concise way of graphically representing the generated VAE model is by means of a heatmap, representing for each sample the average value of each of the latent features. For this we use the HeatMap_Samples() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTCR_U.HeatMap_Samples(filename='heatmap.tif', # Save the figure\n",
    "                       color_dict={'ct1': '#470E61', # Dictionary of class colouring\n",
    "                                  'ct2': '#1F948C',\n",
    "                                  'ct3': '#FDE725',\n",
    "                                  },\n",
    "                        labels= False ) # Supress sample labels in the plot\n",
    "\n",
    "# Plotting the cummulative sum of explained variance of the latent features\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.cumsum(DTCR_U.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. KNN sample classification ###\n",
    "\n",
    "De forma adicional, se ha realizado un K-Nearest-Neighbor classification of the resulting latent features values of each sample retrieved from the previous VAE. El objetivo es evaluar la capacidad predictiva de estas latent features para distinguir los clusters 1, 2 y 3.\n",
    "\n",
    "En nuestro estudio, se ha aplicado KNN y se ha calculado el respectivo AUC, en un modelo entrenado con 50 folds. Se ha aplicado las distancias de KL-Divergence, Euclidean, JS-Divergence, Wasstersein Distance, and Correlation Distance predefinidas en el hiperparámetro distance_metric. Se eligió aquella distancia cuyo AUC por clase fuera el más alto.\n",
    "\n",
    "Por otro lado, dado el alto número de sencuencias a analizar pese a estudiar los 1000 clonotipos más expandidos, se ha subsampleado el número de secuencias a analizar en cada fold a 10000, con el objetivo de que el KNN fuera computacionalmente viable. Para ,ás información, consulte la función KNN_Repertorie_Classifier() en https://sidhomj.github.io/DeepTCR/api/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN with KL-Divergence\n",
    "DTCR_U.KNN_Repertoire_Classifier(metrics=['AUC'],\n",
    "                                distance_metric='KL',\n",
    "                                plot_metrics=True,\n",
    "                                by_class=True, # Show the performance metrics by class\n",
    "                                sample=10000, \n",
    "                                folds=50,  n_jobs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN with correlation\n",
    "DTCR_U.KNN_Repertoire_Classifier(metrics=['AUC'],\n",
    "                                distance_metric='correlation',\n",
    "                                plot_metrics=True,\n",
    "                                by_class=True, \n",
    "                                sample=10000,\n",
    "                                folds=50, n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN with euclidean distance\n",
    "DTCR_U.KNN_Repertoire_Classifier(metrics=['AUC'],\n",
    "                                distance_metric='euclidean',\n",
    "                                plot_metrics=True,\n",
    "                                by_class=True, \n",
    "                                sample=10000,\n",
    "                                folds=50, n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN with Jenssen-Shannon Divergence\n",
    "DTCR_U.KNN_Repertoire_Classifier(metrics=['AUC'],\n",
    "                                distance_metric='JS',\n",
    "                                plot_metrics=True,\n",
    "                                by_class=True, \n",
    "                                sample=10000,\n",
    "                                folds=50, n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN with Wasserstein distance\n",
    "# This method was the one that yielded the highets AUC values for the tree clusters\n",
    "DTCR_U.KNN_Repertoire_Classifier(metrics=['AUC'],\n",
    "                                distance_metric='wasserstein',\n",
    "                                plot_metrics=True,\n",
    "                                by_class=True, \n",
    "                                sample=10000,\n",
    "                                folds=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dptcr2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
